# üìù Agent API

---

Fully monitor your agents with a clean and lightweight API.

---

## üïπÔ∏è Handle Agent State

The Agent API is simple and elegant ‚Äî **just 2 classes, 1 struct and 6 methods**.

To use the `UnityNeuroSpeech Agent API`, you need to:

1. Create a new script.  
2. Add `using UnityNeuroSpeech.Runtime;` at the top.  
3. Derive your class from `AgentBehaviour`.

Once you do that, you must implement four abstract methods:

- `Awake`
- `BeforeTTS`
- `AfterTTS`
- `AfterSTT`

You‚Äôll also need a reference to your `YourAgentNameController` type (in this example, `AlexController`).  
Your script should look like this:

```csharp
using UnityEngine;
using UnityNeuroSpeech.Runtime;

public class AlexBehaviour : AgentBehaviour
{
    [SerializeField] private AlexController _alex;

    public override void Awake() {}
    
    public override void AfterTTS() {}

    public override void BeforeTTS(AgentState state) {}
    
    public override void AfterSTT(string playerMessage) {}
}
```

---

### üîç Methods Overview

---

- **AfterTTS** ‚Äî Called after the audio playback finishes.  
- **BeforeTTS** ‚Äî Called before sending text to the TTS model.  
- **AfterSTT** ‚Äî Called after the STT model finishes transcribing microphone input.  
- **Awake** ‚Äî Works like `MonoBehaviour.Awake()`, but required.  
  Use it to link your behaviour to an agent:

```csharp
public override void Awake() 
{
    AgentManager.SetBehaviourToAgent(_alex, this);
    AgentManager.SetJsonDialogHistory(_alex, "AlexDialogHistory"); 
    // or, if you use encryption:
    AgentManager.SetJsonDialogHistory(_alex, "AlexDialogHistory", "yBIWJczdP7aSbSxB");
}
```

---

### üí° What Is `SetBehaviourToAgent()`?

---

The `SetBehaviourToAgent()` method connects your `AgentBehaviour` to the agent‚Äôs internal event hooks:

```csharp
[HideInInspector] public Action<int, string, string, string> BeforeTTS { get; set; }
[HideInInspector] public Action AfterTTS { get; set; }
[HideInInspector] public Action AfterSTT { get; set; }
```

This ensures UnityNeuroSpeech calls your methods at the correct moments.

---

### üí° What Is `SetJsonDialogHistory()`?

If you use `SetJsonDialogHistory()`, all dialog data between the player and the LLM will be stored in a `.json` file inside `StreamingAssets/`, one file per agent.  
The **second parameter** is the file name (without the `.json` extension).  
The optional **third parameter** is a 16-character AES encryption key.

If encryption is enabled (highly recommended), the player won‚Äôt be able to view dialog history in builds since it‚Äôs encrypted.  
**To decrypt it in the Editor, see the ‚ÄúUseful Tools‚Äù section in the docs.**

---

### üü¢ About AgentState struct:
- responseCount: Number of total replies by the agent.
- emotion: Emotion tag parsed from the LLM response (e.g. "happy", "sad").
- action: Action tag parsed from the LLM response (e.g. "open_door", "play_cutscene_123").
- agentMessage: Raw response from the LLM.
- userPrompt: Player's voice input.

---

‚úÖ Don‚Äôt forget to attach your behaviour script to a `GameObject` in the scene.
