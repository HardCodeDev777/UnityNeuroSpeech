# âš™ï¸ Steps To Make It Work

---

This guide explains only the essential settings.  
You can find tooltips for each field directly in the Unity Editor.

---

## Step 1. ğŸ§ª Settings

---

Go to **UnityNeuroSpeech â†’ Create Settings** in the Unity toolbar.  
Default settings are recommended.  
Donâ€™t forget to click the button (same applies for every step)!

---

## Step 2. ğŸ‘€ UNS Manager

---

**UnityNeuroSpeech Manager** is a GameObject in your scene that controls all non-agent scripts.  
Without it, no agent (talkable AI) will work.

---

Create a `Dropdown` in your scene.  
Then go to **UnityNeuroSpeech â†’ Create UNS Manager**.  
The important setting there is:

- **Whisper model path in StreamingAssets** â€” path to your downloaded Whisper model (`.bin`) inside the `StreamingAssets` folder (without the `Assets` directory).  
  Example:  
  If the full path is  
  `Assets/StreamingAssets/UnityNeuroSpeech/Whisper/ggml-medium.bin`  
  then you should enter  
  `UnityNeuroSpeech/Whisper/ggml-medium.bin`

---

## Step 3. ğŸ§  Agent

---

An **Agent** in UnityNeuroSpeech is a GameObject that can listen, respond, and talk using LLMs.  
**Once you create your first agent, youâ€™ll be able to talk with your AI!**

---

Add a `Button` and an `AudioSource` to your scene.  
Then go to **UnityNeuroSpeech â†’ Create Agent**.  
Here are some important settings:

- **Agent index** â€” the index mentioned in the Quick Start.  
  It links an agent to its voice file.  
  âš ï¸ Each agent must have a unique index!

- **Emotions** â€” the AI can respond with *emotion tags*.  
  Example:  
  `â€“ How are you, DeepSeek?`  
  `â€“ <happy> Iâ€™m feeling grateful. What about you?`  
  The word inside `< >` is the emotion chosen by the AI.  
  Emotions are used for monitoring via the Agent API.  
  The system prompt (generated automatically by UNS) defines how emotions are used.

- **Actions** â€” optional behavior tags like  
  `"turn_off_lights"`, `"enable_cutscene_123"`, `"play_horror_sound"`, etc.

Click **Generate Agent**, then **Create Agent In Scene** â€” *only in that order!*

---

ğŸ‰ Thatâ€™s it! When you run the game:

1. Select a microphone in the dropdown  
2. Click the button to start recording  
3. Speak â†’ click again  
4. **AI responds with voice**

---

After you click **Generate Agent**, two files will be created:  
- `AgentNameController.cs` â€” your agent controller (you donâ€™t need to modify it)  
- `AgentNameSettings.asset` â€” ScriptableObject with agent settings (system prompt, model name, index, etc.)  

You can edit the settings as you wish.  
> Emotions and actions cannot be modified yet â€” stay tuned for updates ğŸ˜

---

Agent performance (â€œspeedâ€) depends on:

- LLM model size  
- Whisper model size  
- Voice file length  
- AI response size  

Small models like **deepseek-r1:7b** or **ggml-tiny.bin** run fast but may ignore system prompts (emotions, actions, etc.).  
Large models like **ggml-large.bin** usually work perfectly â€” but will be slow as hell ğŸ˜

Choose models depending on your goals.  
Is it a problem? Maybe. But it only takes some time of testing to find the perfect setup and build something amazing with this tech.

> On first load, TTS may respond slowly â€” itâ€™s normal. It will work faster next time.