{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installing-requirements","title":"\ud83d\udee0 Installing Requirements","text":"<p>UnityNeuroSpeech requires several programs to be installed. You can simply run <code>setup.bat</code> \u2014 it will download everything automatically. Then just import the <code>.unitypackage</code> into your project.</p>"},{"location":"#what-are-these-requirements","title":"\ud83d\udca1 What Are These Requirements?","text":"<ul> <li>Ollama \u2014 a platform for running large language models (LLMs) locally. You can use models like DeepSeek, Gemma, Qwen, etc. Avoid small models \u2014 they might reduce accuracy and context understanding.</li> <li>UV \u2014 a modern, ultra-fast Python package and environment manager. It replaces traditional tools like <code>pip</code> and <code>venv</code>. Coqui XTTS uses UV to simplify installation and allows running the TTS command directly, without manual Python setup.</li> <li>Coqui XTTS \u2014 a Text-To-Speech model that can generate speech in any custom voice you want: Chester Bennington, Chris Tucker, Vito Corleone (The Godfather), Cyn (Murder Drones), or any other.</li> <li>Whisper \u2014 a Speech-To-Text model. You can use lightweight versions like <code>ggml-tiny.bin</code> for speed, or heavier ones like <code>ggml-medium.bin</code> for better accuracy.</li> </ul>"},{"location":"#voice-files","title":"\ud83c\udf99\ufe0f Voice Files","text":"<p>Don\u2019t forget that you need voice files for AI speech. Make sure your files meet the following requirements:</p> <ul> <li>Format: <code>.wav</code> </li> <li>Duration: 5\u201315 seconds (longer files work, but TTS will load them more slowly)  </li> <li>Contain only one voice and one language, without background noise</li> </ul> <p>Since UnityNeuroSpeech supports multiple voices for multiple agents simultaneously, files must be named correctly: <code>&lt;language&gt;_voice&lt;index&gt;.wav</code></p> <p>Examples:</p> <ol> <li>English voice, agent index <code>0</code> \u2192 <code>en_voice0.wav</code> </li> <li>Russian voice, agent index <code>3</code> \u2192 <code>ru_voice3.wav</code></li> </ol> <p>All voices must be placed in: <code>Assets/StreamingAssets/UnityNeuroSpeech/Voices/</code></p>"},{"location":"#microphone-sprites","title":"\ud83d\uddbc\ufe0f Microphone Sprites","text":"<p>You\u2019ll need two sprites for the microphone state (enabled/disabled). Yes \u2014 without them, it won\u2019t work \ud83e\udd20</p>"},{"location":"unity/agent-api/","title":"\ud83d\udcdd Agent API","text":"<p>Fully control and monitor your agents with a clean and lightweight API.</p>"},{"location":"unity/agent-api/#handle-agent-state","title":"\ud83d\udd79\ufe0f Handle Agent State","text":"<p>The Agent API is simple and elegant \u2014 just 6 methods and 2 classes.</p> <p>To use the <code>UnityNeuroSpeech Agent API</code>, you need to:</p> <ol> <li>Create a new <code>MonoBehaviour</code> script.  </li> <li>Add <code>using UnityNeuroSpeech.Runtime;</code> at the top.  </li> <li>Derive your class from <code>AgentBehaviour</code>.</li> </ol> <p>Once you do that, you must implement four abstract methods:</p> <ul> <li><code>Awake</code></li> <li><code>BeforeTTS</code></li> <li><code>AfterTTS</code></li> <li><code>AfterSTT</code></li> </ul> <p>You\u2019ll also need a reference to your <code>YourAgentNameController</code> type (in this example, <code>AlexController</code>). Your script should look like this:</p> <pre><code>using UnityEngine;\nusing UnityNeuroSpeech.Runtime;\n\npublic class AlexBehaviour : AgentBehaviour\n{\n    [SerializeField] private AlexController _alex;\n\n    public override void Awake() {}\n\n    public override void AfterTTS() {}\n\n    public override void BeforeTTS(int responseCount, string agentMessage, string emotion, string action) {}\n\n    public override void AfterSTT(string playerMessage) {}\n}\n</code></pre>"},{"location":"unity/agent-api/#methods-overview","title":"\ud83d\udd0d Methods Overview","text":"<ul> <li>AfterTTS \u2014 Called after the audio playback finishes.  </li> <li>BeforeTTS \u2014 Called before sending text to the TTS model.  </li> <li>AfterSTT \u2014 Called after the STT model finishes transcribing microphone input.  </li> <li>Awake \u2014 Works like <code>MonoBehaviour.Awake()</code>, but required.   Use it to link your behaviour to an agent:</li> </ul> <pre><code>public override void Awake() \n{\n    AgentManager.SetBehaviourToAgent(_alex, this);\n    AgentManager.SetJsonDialogHistory(_alex, \"AlexDialogHistory\"); \n    // or, if you use encryption:\n    AgentManager.SetJsonDialogHistory(_alex, \"AlexDialogHistory\", \"yBIWJczdP7aSbSxB\");\n}\n</code></pre>"},{"location":"unity/agent-api/#what-is-setbehaviourtoagent","title":"\ud83d\udca1 What Is <code>SetBehaviourToAgent()</code>?","text":"<p>The <code>SetBehaviourToAgent()</code> method connects your <code>AgentBehaviour</code> to the agent\u2019s internal event hooks:</p> <pre><code>[HideInInspector] public Action&lt;int, string, string, string&gt; BeforeTTS { get; set; }\n[HideInInspector] public Action AfterTTS { get; set; }\n[HideInInspector] public Action AfterSTT { get; set; }\n</code></pre> <p>This ensures UnityNeuroSpeech calls your methods at the correct moments.</p>"},{"location":"unity/agent-api/#what-is-setjsondialoghistory","title":"\ud83d\udca1 What Is <code>SetJsonDialogHistory()</code>?","text":"<p>If you use <code>SetJsonDialogHistory()</code>, all dialog data between the player and the LLM will be stored in a <code>.json</code> file inside <code>StreamingAssets/</code>, one file per agent. The second parameter is the file name (without the <code>.json</code> extension). The optional third parameter is a 16-character AES encryption key.</p> <p>If encryption is enabled (highly recommended), the player won\u2019t be able to view dialog history in builds since it\u2019s encrypted. To decrypt it in the Editor, see the \u201cUseful Tools\u201d section in the docs.</p> <p>\u2705 Don\u2019t forget to attach your behaviour script to a <code>GameObject</code> in the scene.</p> <p>\ud83d\ude0e You now have full control over your agents!</p>"},{"location":"unity/steps-to-make-it-work/","title":"\u2699\ufe0f Steps To Make It Work","text":"<p>This guide explains only the essential settings. You can find tooltips for each field directly in the Unity Editor.</p>"},{"location":"unity/steps-to-make-it-work/#step-1-settings","title":"Step 1. \ud83e\uddea Settings","text":"<p>Go to UnityNeuroSpeech \u2192 Create Settings in the Unity toolbar. Default settings are recommended. Don\u2019t forget to click the button (same applies for every step)!</p>"},{"location":"unity/steps-to-make-it-work/#step-2-uns-manager","title":"Step 2. \ud83d\udc40 UNS Manager","text":"<p>UnityNeuroSpeech Manager is a GameObject in your scene that controls all non-agent scripts. Without it, no agent (talkable AI) will work.</p> <p>Create a <code>Dropdown</code> in your scene. Then go to UnityNeuroSpeech \u2192 Create UNS Manager. The important setting there is:</p> <ul> <li>Whisper model path in StreamingAssets \u2014 path to your downloaded Whisper model (<code>.bin</code>) inside the <code>StreamingAssets</code> folder (without the <code>Assets</code> directory).   Example:   If the full path is <code>Assets/StreamingAssets/UnityNeuroSpeech/Whisper/ggml-medium.bin</code>   then you should enter <code>UnityNeuroSpeech/Whisper/ggml-medium.bin</code></li> </ul>"},{"location":"unity/steps-to-make-it-work/#step-3-agent","title":"Step 3. \ud83e\udde0 Agent","text":"<p>An Agent in UnityNeuroSpeech is a GameObject that can listen, respond, and talk using LLMs. Once you create your first agent, you\u2019ll be able to talk with your AI!</p> <p>Add a <code>Button</code> and an <code>AudioSource</code> to your scene. Then go to UnityNeuroSpeech \u2192 Create Agent. Here are some important settings:</p> <ul> <li> <p>Agent index \u2014 the index mentioned in the Quick Start.   It links an agent to its voice file.   \u26a0\ufe0f Each agent must have a unique index!</p> </li> <li> <p>Emotions \u2014 the AI can respond with emotion tags.   Example: <code>\u2013 How are you, DeepSeek?</code> <code>\u2013 &lt;happy&gt; I\u2019m feeling grateful. What about you?</code>   The word inside <code>&lt; &gt;</code> is the emotion chosen by the AI.   Emotions are used for monitoring via the Agent API.   The system prompt (generated automatically by UNS) defines how emotions are used.</p> </li> <li> <p>Actions \u2014 optional behavior tags like <code>\"turn_off_lights\"</code>, <code>\"enable_cutscene_123\"</code>, <code>\"play_horror_sound\"</code>, etc.</p> </li> </ul> <p>Click Generate Agent, then Create Agent In Scene \u2014 only in that order!</p> <p>\ud83c\udf89 That\u2019s it! When you run the game:</p> <ol> <li>Select a microphone in the dropdown  </li> <li>Click the button to start recording  </li> <li>Speak \u2192 click again  </li> <li>AI responds with voice</li> </ol> <p>After you click Generate Agent, two files will be created: - <code>AgentNameController.cs</code> \u2014 your agent controller (you don\u2019t need to modify it) - <code>AgentNameSettings.asset</code> \u2014 ScriptableObject with agent settings (system prompt, model name, index, etc.)  </p> <p>You can edit the settings as you wish.  </p> <p>Emotions and actions cannot be modified yet \u2014 stay tuned for updates \ud83d\ude01</p> <p>Agent performance (\u201cspeed\u201d) depends on:</p> <ul> <li>LLM model size  </li> <li>Whisper model size  </li> <li>Voice file length  </li> <li>AI response size  </li> </ul> <p>Small models like deepseek-r1:7b or ggml-tiny.bin run fast but may ignore system prompts (emotions, actions, etc.). Large models like ggml-large.bin usually work perfectly \u2014 but will be slow as hell \ud83d\ude10</p> <p>Choose models depending on your goals. Is it a problem? Maybe. But it only takes some time of testing to find the perfect setup and build something amazing with this tech.</p> <p>On first load, TTS may respond slowly \u2014 it\u2019s normal. It will work faster next time.</p>"},{"location":"unity/useful-tools/","title":"\ud83d\udee0 Useful Editor Tools","text":"<p>UnityNeuroSpeech provides several Editor tools to make development more convenient.</p>"},{"location":"unity/useful-tools/#prompts-test","title":"\ud83d\uddd2\ufe0f Prompts Test","text":"<p>Let\u2019s say you want to check how a selected LLM model responds to a specific prompt. Normally, you would have to run the game, wait for Whisper to load, say something into the microphone (and risk transcription errors), then wait for the LLM and TTS to finish \u2014 quite the hardcore workflow, right?</p> <p>This tool allows you to test prompts instantly. You only wait for the LLM (as usual) to generate a response \u2014 and you can even see the generation time in milliseconds!</p> <p>To access it, go to UnityNeuroSpeech \u2192 Prompts Test.</p>"},{"location":"unity/useful-tools/#decode-encoded","title":"\ud83d\udd75\ufe0f\u200d\u2642\ufe0f Decode Encoded","text":"<p>If you use AES encryption, your <code>.json</code> dialog history files will be encrypted. But what if you want to view their contents? This tool lets you decrypt and read them easily.</p> <p>To access it, go to UnityNeuroSpeech \u2192 Decode Encoded.</p> <p>Note about the Key to encrypt field: You must use the same key you specified in your <code>AgentBehaviour</code> script.</p>"}]}